var documenterSearchIndex = {"docs":
[{"location":"guide/#User-Guide","page":"User Guide","title":"User Guide","text":"This guide covers the essential workflows for using MultiGridBarrierPETSc.jl.","category":"section"},{"location":"guide/#Initialization","page":"User Guide","title":"Initialization","text":"Every program using MultiGridBarrierPETSc.jl must initialize the package before calling any functions:\n\nusing MultiGridBarrierPETSc\nMultiGridBarrierPETSc.Init()  # Initialize MPI and PETSc before calling functions\n\nThe Init() function:\n\nInitializes MPI and PETSc if not already initialized\nConfigures MUMPS sparse direct solver for accurate linear solves\nOnly needs to be called once at the start of your program","category":"section"},{"location":"guide/#Basic-Workflow","page":"User Guide","title":"Basic Workflow","text":"The typical workflow consists of three steps:\n\nSolve with PETSc types (distributed computation)\nConvert to native types (for analysis/plotting)\nVisualize or analyze (using MultiGridBarrier's tools)","category":"section"},{"location":"guide/#Complete-Example-with-Visualization","page":"User Guide","title":"Complete Example with Visualization","text":"Here's a complete example that solves a 2D FEM problem, converts the solution, and plots it:\n\nusing MultiGridBarrierPETSc\nusing MultiGridBarrier\nusing PyPlot\nMultiGridBarrierPETSc.Init()\n\n# Step 1: Solve with PETSc distributed types (L=3 for fast documentation builds)\nsol_petsc = fem2d_petsc_solve(Float64; L=3, p=1.0, verbose=false)\n\n# Step 2: Convert solution to native Julia types\nsol_native = petsc_to_native(sol_petsc)\n\n# Step 3: Plot the solution using MultiGridBarrier's plot function\nfigure(figsize=(10, 8))\nplot(sol_native)\ntitle(\"Multigrid Barrier Solution (L=3)\")\ntight_layout()\nsavefig(\"solution_plot.png\")\nprintln(io0(), \"Solution plotted!\")\n\ntip: Running This Example\nSave this code to a file (e.g., visualize.jl) and run with:julia -e 'using MPI; run(`$(MPI.mpiexec()) -n 4 $(Base.julia_cmd()) visualize.jl`)'This uses MPI.mpiexec() to get the correct MPI launcher configured for your Julia installation, avoiding compatibility issues with system mpiexec. Add --project or other Julia options as needed for your environment.This will create solution_plot.png showing the computed solution.","category":"section"},{"location":"guide/#Understanding-MPI-Collective-Operations","page":"User Guide","title":"Understanding MPI Collective Operations","text":"warning: All Functions Are Collective\nAll exported functions in MultiGridBarrierPETSc.jl are MPI collective operations. This means:All MPI ranks must call the function\nAll ranks must call it with the same parameters\nDeadlock will occur if only some ranks call a collective function\n\nCorrect usage:\n\n# All ranks execute this together\nsol = fem2d_petsc_solve(Float64; L=2, p=1.0)\n\nIncorrect usage (causes deadlock):\n\nrank = MPI.Comm_rank(MPI.COMM_WORLD)\nif rank == 0\n    sol = fem2d_petsc_solve(Float64; L=2, p=1.0)  # ✗ Only rank 0 calls - DEADLOCK!\nend","category":"section"},{"location":"guide/#Type-Conversions","page":"User Guide","title":"Type Conversions","text":"","category":"section"},{"location":"guide/#Native-to-PETSc","page":"User Guide","title":"Native to PETSc","text":"Convert native Julia arrays to PETSc distributed types:\n\nusing MultiGridBarrier\n\n# Create native geometry\ng_native = fem2d(; maxh=0.3)\n\n# Convert to PETSc types for distributed computation\ng_petsc = native_to_petsc(g_native)\n\nType mappings:\n\nNative Type PETSc Type Storage\nMatrix{T} Mat{T, MPIDENSE} Dense distributed\nVector{T} Vec{T} Dense distributed\nSparseMatrixCSC{T,Int} Mat{T, MPIAIJ} Sparse distributed","category":"section"},{"location":"guide/#PETSc-to-Native","page":"User Guide","title":"PETSc to Native","text":"Convert PETSc types back to native Julia arrays:\n\n# Create and solve with PETSc types\ng_petsc = fem2d_petsc(Float64; maxh=0.3)\nsol_petsc = amgb(g_petsc; p=2.0)\n\n# Convert back for analysis\ng_native = petsc_to_native(g_petsc)\nsol_native = petsc_to_native(sol_petsc)\n\n# Now you can use native Julia operations\nusing LinearAlgebra\nz_matrix = sol_native.z\nsolution_norm = norm(z_matrix)\nprintln(io0(), \"Solution norm: \", solution_norm)","category":"section"},{"location":"guide/#Advanced-Usage","page":"User Guide","title":"Advanced Usage","text":"","category":"section"},{"location":"guide/#Custom-Geometry-Workflow","page":"User Guide","title":"Custom Geometry Workflow","text":"For more control, construct geometries manually:\n\nusing MultiGridBarrierPETSc\nusing MultiGridBarrier\nMultiGridBarrierPETSc.Init()\n\n# 1. Create native geometry with specific parameters\ng_native = fem2d(; maxh=0.2, L=2)\n\n# 2. Convert to PETSc for distributed solving\ng_petsc = native_to_petsc(g_native)\n\n# 3. Solve with custom barrier parameters\nsol_petsc = amgb(g_petsc;\n    p=1.5,           # Barrier power parameter\n    verbose=true,    # Print convergence info\n    maxit=100,       # Maximum iterations\n    tol=1e-8)        # Convergence tolerance\n\n# 4. Convert solution back\nsol_native = petsc_to_native(sol_petsc)\n\n# 5. Access solution components\nprintln(io0(), \"Newton steps: \", sum(sol_native.SOL_main.its))\nprintln(io0(), \"Elapsed time: \", sol_native.SOL_main.t_elapsed, \" seconds\")","category":"section"},{"location":"guide/#Comparing-PETSc-vs-Native-Solutions","page":"User Guide","title":"Comparing PETSc vs Native Solutions","text":"Verify that PETSc and native implementations give the same results:\n\nusing MultiGridBarrierPETSc\nusing MultiGridBarrier\nusing LinearAlgebra\nMultiGridBarrierPETSc.Init()\n\n# Solve with PETSc (distributed)\nsol_petsc_dist = fem2d_petsc_solve(Float64; L=2, p=1.0, verbose=false)\nz_petsc = petsc_to_native(sol_petsc_dist).z\n\n# Solve with native (sequential, on rank 0)\nrank = MPI.Comm_rank(MPI.COMM_WORLD)\nif rank == 0\n    sol_native = MultiGridBarrier.fem2d_solve(Float64; L=2, p=1.0, verbose=false)\n    z_native = sol_native.z\n\n    # Compare solutions\n    diff = norm(z_petsc - z_native) / norm(z_native)\n    println(\"Relative difference: \", diff)\n    @assert diff < 1e-10 \"Solutions should match!\"\nend","category":"section"},{"location":"guide/#IO-and-Output","page":"User Guide","title":"IO and Output","text":"","category":"section"},{"location":"guide/#Printing-from-One-Rank","page":"User Guide","title":"Printing from One Rank","text":"Use io0() to print from rank 0 only:\n\nusing SafePETSc\n\n# This prints once (from rank 0)\nprintln(io0(), \"Hello from rank 0!\")\n\n# Without io0(), this prints from ALL ranks\nprintln(\"Hello from rank \", MPI.Comm_rank(MPI.COMM_WORLD))","category":"section"},{"location":"guide/#Displaying-PETSc-Types","page":"User Guide","title":"Displaying PETSc Types","text":"PETSc Vec and Mat types implement show() methods, but these are collective:\n\nusing SafePETSc  # For io0()\nusing MultiGridBarrierPETSc\nMultiGridBarrierPETSc.Init()\n\ng = fem2d_petsc(Float64; maxh=0.5)\n\n# Collective operation - all ranks participate\nprintln(io0(), g.w)  # Prints the Vec once on rank 0","category":"section"},{"location":"guide/#Performance-Considerations","page":"User Guide","title":"Performance Considerations","text":"","category":"section"},{"location":"guide/#Mesh-Size-and-MPI-Ranks","page":"User Guide","title":"Mesh Size and MPI Ranks","text":"For efficient parallel computation:\n\nSmall problems (L ≤ 3): Use 1-4 MPI ranks\nMedium problems (L = 4-5): Use 4-16 MPI ranks\nLarge problems (L ≥ 6): Use 16+ MPI ranks","category":"section"},{"location":"guide/#MUMPS-Solver-Configuration","page":"User Guide","title":"MUMPS Solver Configuration","text":"The MUMPS sparse direct solver is configured automatically during Init():\n\nMultiGridBarrierPETSc.Init()  # Prints \"Initializing MultiGridBarrierPETSc with solver options...\"\n\nMUMPS provides exact sparse direct solves for MPIAIJ (sparse) matrices, ensuring accurate Newton iterations in the barrier method.","category":"section"},{"location":"guide/#Common-Patterns","page":"User Guide","title":"Common Patterns","text":"","category":"section"},{"location":"guide/#Solve-and-Extract-Specific-Values","page":"User Guide","title":"Solve and Extract Specific Values","text":"using SafePETSc  # For io0()\nusing MultiGridBarrierPETSc\nMultiGridBarrierPETSc.Init()\n\nsol = fem2d_petsc_solve(Float64; L=3, p=1.0)\nsol_native = petsc_to_native(sol)\n\n# Access solution data\nz = sol_native.z  # Solution matrix\niters = sum(sol_native.SOL_main.its)  # Total Newton steps\nelapsed = sol_native.SOL_main.t_elapsed  # Elapsed time in seconds\n\nprintln(io0(), \"Converged in $iters iterations\")\nprintln(io0(), \"Elapsed time: $elapsed seconds\")","category":"section"},{"location":"guide/#1D-Problems","page":"User Guide","title":"1D Problems","text":"MultiGridBarrierPETSc supports 1D finite element problems through MultiGridBarrier.jl.","category":"section"},{"location":"guide/#Basic-1D-Example","page":"User Guide","title":"Basic 1D Example","text":"using MultiGridBarrierPETSc\nusing SafePETSc  # For io0()\nMultiGridBarrierPETSc.Init()\n\n# Solve a 1D problem with 4 multigrid levels (2^4 = 16 elements)\nsol = fem1d_petsc_solve(Float64; L=4, p=1.0, verbose=true)\n\n# Convert solution to native types for analysis\nsol_native = petsc_to_native(sol)\n\nprintln(io0(), \"Solution computed successfully!\")\nprintln(io0(), \"Newton steps: \", sum(sol_native.SOL_main.its))","category":"section"},{"location":"guide/#1D-Geometry-Creation","page":"User Guide","title":"1D Geometry Creation","text":"For more control, create the geometry separately:\n\nusing MultiGridBarrierPETSc\nusing MultiGridBarrier\nMultiGridBarrierPETSc.Init()\n\n# Create 1D PETSc geometry\ng = fem1d_petsc(Float64; L=4)\n\n# Solve with custom parameters\nsol = amgb(g;\n    p=1.0,           # Barrier power parameter\n    verbose=true,    # Print convergence info\n    maxit=100)       # Maximum iterations\n\n# Convert solution back to native types\nsol_native = petsc_to_native(sol)","category":"section"},{"location":"guide/#1D-Parameters","page":"User Guide","title":"1D Parameters","text":"The fem1d_petsc and fem1d_petsc_solve functions accept:\n\nParameter Description Default\nL Number of multigrid levels (creates 2^L elements) 4","category":"section"},{"location":"guide/#Comparing-1D-PETSc-vs-Native-Solutions","page":"User Guide","title":"Comparing 1D PETSc vs Native Solutions","text":"using MultiGridBarrierPETSc\nusing MultiGridBarrier\nusing LinearAlgebra\nusing SafePETSc  # For io0()\nMultiGridBarrierPETSc.Init()\n\n# Solve with PETSc (distributed)\nsol_petsc = fem1d_petsc_solve(Float64; L=4, p=1.0, verbose=false)\nz_petsc = petsc_to_native(sol_petsc).z\n\n# Solve with native (sequential)\nsol_native = MultiGridBarrier.fem1d_solve(Float64; L=4, p=1.0, verbose=false)\nz_native = sol_native.z\n\n# Compare solutions\ndiff = norm(z_petsc - z_native) / norm(z_native)\nprintln(io0(), \"Relative difference: \", diff)","category":"section"},{"location":"guide/#3D-Problems","page":"User Guide","title":"3D Problems","text":"MultiGridBarrierPETSc also supports 3D hexahedral finite elements through MultiGridBarrier3d.jl.","category":"section"},{"location":"guide/#Basic-3D-Example","page":"User Guide","title":"Basic 3D Example","text":"using MultiGridBarrierPETSc\nusing SafePETSc  # For io0()\nMultiGridBarrierPETSc.Init()\n\n# Solve a 3D problem with Q3 elements and 2 multigrid levels\nsol = fem3d_petsc_solve(Float64; L=2, k=3, p=1.0, verbose=true)\n\n# Convert solution to native types for analysis\nsol_native = petsc_to_native(sol)\n\nprintln(io0(), \"Solution computed successfully!\")\nprintln(io0(), \"Newton steps: \", sum(sol_native.SOL_main.its))","category":"section"},{"location":"guide/#3D-Geometry-Creation","page":"User Guide","title":"3D Geometry Creation","text":"For more control, create the geometry separately:\n\nusing MultiGridBarrierPETSc\nusing MultiGridBarrier\nMultiGridBarrierPETSc.Init()\n\n# Create 3D PETSc geometry\ng = fem3d_petsc(Float64; L=2, k=3)\n\n# Solve with custom parameters\nsol = amgb(g;\n    p=1.0,           # Barrier power parameter\n    verbose=true,    # Print convergence info\n    maxit=100)       # Maximum iterations\n\n# Convert solution back to native types\nsol_native = petsc_to_native(sol)","category":"section"},{"location":"guide/#3D-Parameters","page":"User Guide","title":"3D Parameters","text":"The fem3d_petsc and fem3d_petsc_solve functions accept:\n\nParameter Description Default\nL Number of multigrid levels 2\nk Polynomial order of elements (Q_k) 3\nK Coarse Q1 mesh (N×3 matrix, 8 vertices per hex) Unit cube [-1,1]³","category":"section"},{"location":"guide/#Comparing-3D-PETSc-vs-Native-Solutions","page":"User Guide","title":"Comparing 3D PETSc vs Native Solutions","text":"using MultiGridBarrierPETSc\nusing MultiGridBarrier3d\nusing LinearAlgebra\nusing SafePETSc  # For io0()\nMultiGridBarrierPETSc.Init()\n\n# Solve with PETSc (distributed)\nsol_petsc = fem3d_petsc_solve(Float64; L=2, k=2, p=1.0, verbose=false)\nz_petsc = petsc_to_native(sol_petsc).z\n\n# Solve with native (sequential)\nsol_native = MultiGridBarrier3d.fem3d_solve(Float64; L=2, k=2, p=1.0, verbose=false)\nz_native = sol_native.z\n\n# Compare solutions\ndiff = norm(z_petsc - z_native) / norm(z_native)\nprintln(io0(), \"Relative difference: \", diff)","category":"section"},{"location":"guide/#Next-Steps","page":"User Guide","title":"Next Steps","text":"See the API Reference for detailed function documentation\nCheck the examples/ directory for complete runnable examples\nConsult MultiGridBarrier.jl documentation for barrier method theory\nConsult MultiGridBarrier3d.jl documentation for 3D FEM details","category":"section"},{"location":"api/#API-Reference","page":"API Reference","title":"API Reference","text":"This page provides detailed documentation for all exported functions in MultiGridBarrierPETSc.jl.\n\nnote: All Functions Are Collective\nAll functions documented here are MPI collective operations. Every MPI rank must call these functions together with the same parameters. Failure to do so will result in deadlock.","category":"section"},{"location":"api/#Initialization","page":"API Reference","title":"Initialization","text":"This function must be called before using any other MultiGridBarrierPETSc functionality.","category":"section"},{"location":"api/#High-Level-API","page":"API Reference","title":"High-Level API","text":"These functions provide the simplest interface for solving problems with PETSc types.","category":"section"},{"location":"api/#1D-Problems","page":"API Reference","title":"1D Problems","text":"","category":"section"},{"location":"api/#2D-Problems","page":"API Reference","title":"2D Problems","text":"","category":"section"},{"location":"api/#3D-Problems","page":"API Reference","title":"3D Problems","text":"","category":"section"},{"location":"api/#Type-Conversion-API","page":"API Reference","title":"Type Conversion API","text":"These functions convert between native Julia types and PETSc distributed types. The petsc_to_native function dispatches on type, handling both Geometry and AMGBSOL objects.","category":"section"},{"location":"api/#Type-Mappings-Reference","page":"API Reference","title":"Type Mappings Reference","text":"","category":"section"},{"location":"api/#Native-to-PETSc-Conversions","page":"API Reference","title":"Native to PETSc Conversions","text":"When converting from native Julia types to PETSc distributed types:\n\nNative Type PETSc Type PETSc Prefix Usage\nMatrix{T} Mat{T, MPIDENSE} MPIDENSE Geometry coordinates, dense operators\nVector{T} Vec{T} - Weights, dense vectors\nSparseMatrixCSC{T,Int} Mat{T, MPIAIJ} MPIAIJ Sparse operators, subspace matrices","category":"section"},{"location":"api/#PETSc-to-Native-Conversions","page":"API Reference","title":"PETSc to Native Conversions","text":"When converting from PETSc distributed types back to native Julia types:\n\nPETSc Type Native Type Conversion Method\nMat{T, MPIDENSE} Matrix{T} SafePETSc.J()\nMat{T, MPIAIJ} SparseMatrixCSC{T,Int} SafePETSc.J()\nVec{T} Vector{T} SafePETSc.J()","category":"section"},{"location":"api/#Geometry-Structure","page":"API Reference","title":"Geometry Structure","text":"The Geometry type from MultiGridBarrier is parameterized by its storage types:\n\nNative Geometry:\n\nGeometry{T, Matrix{T}, Vector{T}, SparseMatrixCSC{T,Int}, Discretization}\n\nPETSc Geometry:\n\nGeometry{T, Mat{T,MPIDENSE}, Vec{T}, Mat{T,MPIAIJ}, Discretization}","category":"section"},{"location":"api/#Fields","page":"API Reference","title":"Fields","text":"discretization: Discretization information (domain, mesh, etc.)\nx: Geometry coordinates (Matrix or Mat)\nw: Quadrature weights (Vector or Vec)\noperators: Dictionary of operators (id, laplacian, mass, etc.)\nsubspaces: Dictionary of subspace projection matrices\nrefine: Vector of refinement matrices (coarse → fine)\ncoarsen: Vector of coarsening matrices (fine → coarse)","category":"section"},{"location":"api/#Solution-Structure","page":"API Reference","title":"Solution Structure","text":"The AMGBSOL type from MultiGridBarrier contains the complete solution:","category":"section"},{"location":"api/#Fields-2","page":"API Reference","title":"Fields","text":"z: Solution matrix/vector\nSOL_feasibility: NamedTuple with feasibility phase information\nSOL_main: NamedTuple with main solve information\nt_elapsed: Elapsed solve time in seconds\nts: Barrier parameter values\nits: Iterations per level\nc_dot_Dz: Convergence measure values\nlog: Vector of iteration logs\ngeometry: The geometry used for solving","category":"section"},{"location":"api/#MPI-and-IO-Utilities","page":"API Reference","title":"MPI and IO Utilities","text":"","category":"section"},{"location":"api/#SafePETSc.io0()","page":"API Reference","title":"SafePETSc.io0()","text":"Returns an IO stream that only writes on rank 0:\n\nprintln(io0(), \"This prints once from rank 0\")\nprintln(io0(), my_petsc_vec)  # Collective show() of Vec","category":"section"},{"location":"api/#MPI-Rank-Information","page":"API Reference","title":"MPI Rank Information","text":"using MPI\n\nrank = MPI.Comm_rank(MPI.COMM_WORLD)  # Current rank (0 to nranks-1)\nnranks = MPI.Comm_size(MPI.COMM_WORLD)  # Total number of ranks","category":"section"},{"location":"api/#PETSc-Configuration","page":"API Reference","title":"PETSc Configuration","text":"","category":"section"},{"location":"api/#MUMPS-Solver","page":"API Reference","title":"MUMPS Solver","text":"The Init() function automatically configures PETSc to use MUMPS for sparse matrices:\n\n# Equivalent PETSc options set automatically:\n# -MPIAIJ_ksp_type preonly          # No iterative solver, just direct solve\n# -MPIAIJ_pc_type lu                # LU factorization\n# -MPIAIJ_pc_factor_mat_solver_type mumps  # Use MUMPS for factorization\n\nMatrix Type Configuration:\n\nSparse matrices (MPIAIJ): Use MUMPS direct solver for exact solves\nDense matrices (MPIDENSE): Used only for geometry data (coordinates and weights), not for linear solves\n\nThis ensures exact direct solves for the sparse linear systems in the barrier method's Newton iterations.","category":"section"},{"location":"api/#Examples","page":"API Reference","title":"Examples","text":"","category":"section"},{"location":"api/#Type-Conversion-Round-Trip","page":"API Reference","title":"Type Conversion Round-Trip","text":"using MultiGridBarrierPETSc\nusing MultiGridBarrier\nusing LinearAlgebra\nMultiGridBarrierPETSc.Init()\n\n# Create native geometry\ng_native = fem2d(; maxh=0.3)\n\n# Convert to PETSc\ng_petsc = native_to_petsc(g_native)\n\n# Solve with PETSc types\nsol_petsc = amgb(g_petsc; p=2.0)\n\n# Convert back to native\nsol_native = petsc_to_native(sol_petsc)\ng_back = petsc_to_native(g_petsc)\n\n# Verify round-trip accuracy\n@assert norm(g_native.x - g_back.x) < 1e-10\n@assert norm(g_native.w - g_back.w) < 1e-10","category":"section"},{"location":"api/#Accessing-Operator-Matrices","page":"API Reference","title":"Accessing Operator Matrices","text":"# Native geometry\ng_native = fem2d(; maxh=0.2)\nlap_native = g_native.operators[:laplacian]  # SparseMatrixCSC\n\n# PETSc geometry\ng_petsc = native_to_petsc(g_native)\nlap_petsc = g_petsc.operators[:laplacian]  # Mat{Float64, MPIAIJ}\n\n# Convert back if needed\nlap_back = SafePETSc.J(lap_petsc)  # SparseMatrixCSC","category":"section"},{"location":"api/#Integration-with-MultiGridBarrier","page":"API Reference","title":"Integration with MultiGridBarrier","text":"All MultiGridBarrier functions work seamlessly with PETSc types:\n\nusing MultiGridBarrier: amgb, amgb_solve\n\n# Create PETSc geometry\ng = fem2d_petsc(Float64; L=3)\n\n# Use MultiGridBarrier functions directly\nsol = amgb(g; p=1.0, verbose=true)\nsol = amgb_solve(g; p=1.5, maxit=50, tol=1e-10)\n\nThe package extends MultiGridBarrier's internal API (amgb_zeros, amgb_hcat, amgb_diag, etc.) to work with PETSc types automatically.","category":"section"},{"location":"api/#MultiGridBarrierPETSc.Init","page":"API Reference","title":"MultiGridBarrierPETSc.Init","text":"Init(; options=\"-MPIAIJ_ksp_type preonly -MPIAIJ_pc_type lu -MPIAIJ_pc_factor_mat_solver_type mumps\")\n\nCollective\n\nInitialize MultiGridBarrierPETSc by setting up MPI, PETSc, and solver options.\n\nThis function should be called once before using any MultiGridBarrierPETSc functionality. It will:\n\nInitialize MPI and PETSc if not already initialized\nConfigure PETSc solver options (default: use MUMPS direct solver for sparse matrices)\n\nThe default options configure MPIAIJ (sparse) matrices to use:\n\n-ksp_type preonly: Don't use iterative solver, just apply preconditioner\n-pc_type lu: Use LU factorization as preconditioner\n-pc_factor_mat_solver_type mumps: Use MUMPS sparse direct solver for the factorization\n\nNote: MPIDENSE matrices are only used for geometry data (coordinates and weights), not for linear solves.\n\nArguments\n\noptions::String: PETSc options string to set (default: MUMPS direct solver for sparse matrices)\n\nExample\n\nusing MultiGridBarrierPETSc\nMultiGridBarrierPETSc.Init()  # Use default MUMPS solver for sparse matrices\n\n# Or with custom options:\nMultiGridBarrierPETSc.Init(options=\"-MPIAIJ_ksp_type cg -MPIAIJ_pc_type jacobi\")\n\n\n\n\n\n","category":"function"},{"location":"api/#MultiGridBarrierPETSc.fem1d_petsc","page":"API Reference","title":"MultiGridBarrierPETSc.fem1d_petsc","text":"fem1d_petsc(::Type{T}=Float64; kwargs...) where {T}\n\nCollective\n\nCreate a PETSc-based Geometry from fem1d parameters.\n\nThis function calls fem1d(kwargs...) to create a native 1D geometry, then converts it to use PETSc distributed types (Mat and Vec) for distributed computing.\n\nNote: Call MultiGridBarrierPETSc.Init() before using this function.\n\nArguments\n\nT::Type: Element type for the geometry (default: Float64)\nkwargs...: Additional keyword arguments passed to fem1d():\nL::Int: Number of multigrid levels (default: 4), creating 2^L elements\n\nReturns\n\nA Geometry object with PETSc distributed types.\n\nExample\n\nMultiGridBarrierPETSc.Init()\ng = fem1d_petsc(Float64; L=4)\n\n\n\n\n\n","category":"function"},{"location":"api/#MultiGridBarrierPETSc.fem1d_petsc_solve","page":"API Reference","title":"MultiGridBarrierPETSc.fem1d_petsc_solve","text":"fem1d_petsc_solve(::Type{T}=Float64; kwargs...) where {T}\n\nCollective\n\nSolve a fem1d problem using amgb with PETSc distributed types.\n\nThis is a convenience function that combines fem1d_petsc and amgb into a single call. It creates a PETSc-based 1D geometry and solves the barrier problem.\n\nArguments\n\nT::Type: Element type for the geometry (default: Float64)\nkwargs...: Keyword arguments passed to both fem1d_petsc and amgb\nL::Int: Number of multigrid levels (passed to fem1d)\np: Power parameter for the barrier (passed to amgb)\nverbose: Verbosity flag (passed to amgb)\nOther arguments specific to fem1d or amgb\n\nReturns\n\nThe solution object from amgb.\n\nExample\n\nsol = fem1d_petsc_solve(Float64; L=4, p=1.0, verbose=true)\nprintln(\"Solution norm: \", norm(sol.z))\n\n\n\n\n\n","category":"function"},{"location":"api/#MultiGridBarrierPETSc.fem2d_petsc","page":"API Reference","title":"MultiGridBarrierPETSc.fem2d_petsc","text":"fem2d_petsc(::Type{T}=Float64; kwargs...) where {T}\n\nCollective\n\nCreate a PETSc-based Geometry from fem2d parameters.\n\nThis function calls fem2d(kwargs...) to create a native geometry, then converts it to use PETSc distributed types (Mat and Vec) for distributed computing.\n\nNote: Call MultiGridBarrierPETSc.Init() before using this function.\n\nArguments\n\nT::Type: Element type for the geometry (default: Float64)\nkwargs...: Additional keyword arguments passed to fem2d()\n\nReturns\n\nA Geometry object with PETSc distributed types.\n\nExample\n\nMultiGridBarrierPETSc.Init()\ng = fem2d_petsc(Float64; maxh=0.1)\n\n\n\n\n\n","category":"function"},{"location":"api/#MultiGridBarrierPETSc.fem2d_petsc_solve","page":"API Reference","title":"MultiGridBarrierPETSc.fem2d_petsc_solve","text":"fem2d_petsc_solve(::Type{T}=Float64; kwargs...) where {T}\n\nCollective\n\nSolve a fem2d problem using amgb with PETSc distributed types.\n\nThis is a convenience function that combines fem2d_petsc and amgb into a single call. It creates a PETSc-based geometry and solves the barrier problem.\n\nArguments\n\nT::Type: Element type for the geometry (default: Float64)\nkwargs...: Keyword arguments passed to both fem2d_petsc and amgb\nmaxh: Maximum mesh size (passed to fem2d)\np: Power parameter for the barrier (passed to amgb)\nverbose: Verbosity flag (passed to amgb)\nOther arguments specific to fem2d or amgb\n\nReturns\n\nThe solution object from amgb.\n\nExample\n\nsol = fem2d_petsc_solve(Float64; maxh=0.1, p=2.0, verbose=true)\nprintln(\"Solution norm: \", norm(sol.z))\n\n\n\n\n\n","category":"function"},{"location":"api/#MultiGridBarrierPETSc.fem3d_petsc","page":"API Reference","title":"MultiGridBarrierPETSc.fem3d_petsc","text":"fem3d_petsc(::Type{T}=Float64; kwargs...) where {T}\n\nCollective\n\nCreate a PETSc-based Geometry from fem3d parameters.\n\nThis function calls fem3d(kwargs...) to create a native 3D geometry, then converts it to use PETSc distributed types (Mat and Vec) for distributed computing.\n\nNote: Call MultiGridBarrierPETSc.Init() before using this function.\n\nArguments\n\nT::Type: Element type for the geometry (default: Float64)\nkwargs...: Additional keyword arguments passed to fem3d():\nL::Int: Number of multigrid levels (default: 2)\nk::Int: Polynomial order of elements (default: 3)\nK: Coarse Q1 mesh as an N×3 matrix (optional, defaults to unit cube)\n\nReturns\n\nA Geometry object with PETSc distributed types.\n\nExample\n\nMultiGridBarrierPETSc.Init()\ng = fem3d_petsc(Float64; L=2, k=3)\n\n\n\n\n\n","category":"function"},{"location":"api/#MultiGridBarrierPETSc.fem3d_petsc_solve","page":"API Reference","title":"MultiGridBarrierPETSc.fem3d_petsc_solve","text":"fem3d_petsc_solve(::Type{T}=Float64; kwargs...) where {T}\n\nCollective\n\nSolve a fem3d problem using amgb with PETSc distributed types.\n\nThis is a convenience function that combines fem3d_petsc and amgb into a single call. It creates a PETSc-based 3D geometry and solves the barrier problem.\n\nArguments\n\nT::Type: Element type for the geometry (default: Float64)\nkwargs...: Keyword arguments passed to both fem3d_petsc and amgb\nL::Int: Number of multigrid levels (passed to fem3d)\nk::Int: Polynomial order of elements (passed to fem3d)\np: Power parameter for the barrier (passed to amgb)\nverbose: Verbosity flag (passed to amgb)\nD: Operator structure matrix (passed to amgb, defaults to 3D operators)\nf: Source term function (passed to amgb, defaults to 3D source)\ng: Boundary condition function (passed to amgb, defaults to 3D BCs)\nOther arguments specific to fem3d or amgb\n\nReturns\n\nThe solution object from amgb.\n\nExample\n\nsol = fem3d_petsc_solve(Float64; L=2, k=3, p=1.0, verbose=true)\nprintln(\"Solution norm: \", norm(sol.z))\n\n\n\n\n\n","category":"function"},{"location":"api/#MultiGridBarrierPETSc.native_to_petsc","page":"API Reference","title":"MultiGridBarrierPETSc.native_to_petsc","text":"native_to_petsc(g_native::Geometry{T, Matrix{T}, Vector{T}, SparseMatrixCSC{T,Int}, Discretization}) where {T, Discretization}\n\nCollective\n\nConvert a native Geometry object (with Julia arrays) to use PETSc distributed types.\n\nThis is a collective operation. Each rank calls fem2d() to get the same native geometry, then this function converts:\n\nx::Matrix{T} -> x::Mat{T, MPIDENSE}\nw::Vector{T} -> w::Vec{T}\noperators[key]::SparseMatrixCSC{T,Int} -> operators[key]::Mat{T, MPIAIJ}\nsubspaces[key][i]::SparseMatrixCSC{T,Int} -> subspaces[key][i]::Mat{T, MPIAIJ}\n\nThe MPIDENSE prefix indicates dense storage (for geometry data and weights), while MPIAIJ indicates sparse storage (for operators and subspace matrices).\n\n\n\n\n\n","category":"function"},{"location":"api/#MultiGridBarrierPETSc.petsc_to_native","page":"API Reference","title":"MultiGridBarrierPETSc.petsc_to_native","text":"petsc_to_native(g_petsc::Geometry{T, Mat{T,XPrefix}, Vec{T}, Mat{T,MPrefix}, Discretization}) where {T, XPrefix, MPrefix, Discretization}\n\nCollective\n\nConvert a PETSc Geometry object (with distributed PETSc types) back to native Julia arrays.\n\nThis is a collective operation. This function converts:\n\nx::Mat{T, MPIDENSE} -> x::Matrix{T}\nw::Vec{T} -> w::Vector{T}\noperators[key]::Mat{T, MPIAIJ} -> operators[key]::SparseMatrixCSC{T,Int}\nsubspaces[key][i]::Mat{T, MPIAIJ} -> subspaces[key][i]::SparseMatrixCSC{T,Int}\n\nUses SafePETSc.J() which automatically handles dense vs sparse conversion based on the Mat's storage type.\n\n\n\n\n\npetsc_to_native(sol_petsc::AMGBSOL{T, XType, WType, MType, Discretization}) where {T, XType, WType, MType, Discretization}\n\nCollective\n\nConvert an AMGBSOL solution object from PETSc types back to native Julia types.\n\nThis is a collective operation that performs a deep conversion of the solution structure:\n\nz: Mat{T,Prefix} -> Matrix{T} or Vec{T} -> Vector{T} (depending on the type)\nSOL_feasibility: NamedTuple with PETSc types -> NamedTuple with native types\nSOL_main: NamedTuple with PETSc types -> NamedTuple with native types\ngeometry: Geometry with PETSc types -> Geometry with native types\n\nUses SafePETSc.J() which automatically handles dense vs sparse conversion based on the Mat's storage type.\n\n\n\n\n\n","category":"function"},{"location":"installation/#Installation","page":"Installation","title":"Installation","text":"","category":"section"},{"location":"installation/#Prerequisites","page":"Installation","title":"Prerequisites","text":"","category":"section"},{"location":"installation/#MPI-and-PETSc","page":"Installation","title":"MPI and PETSc","text":"For most users, no manual installation is required. When you install MultiGridBarrierPETSc.jl, Julia automatically provides:\n\nMPI.jl with MPI_jll (bundled MPI implementation)\nPETSc.jl with PETSc_jll (precompiled PETSc binary)\n\nThis works out of the box on most systems.\n\nnote: MUMPS Support\nMultiGridBarrierPETSc.jl uses the MUMPS direct solver. The bundled PETSc_jll includes MUMPS on macOS, but may not on some Linux distributions. If you encounter solver errors, you may need a custom PETSc build (see below).\n\nHPC and Custom Builds\n\nFor high-performance computing environments or when the default PETSc_jll lacks MUMPS support, follow the official PETSc.jl configuration guide to link to an external PETSc installation.\n\nLinux Build Considerations\n\nOn some Linux platforms, there are known incompatibilities between Julia's bundled libraries and PETSc's optional HDF5 and curl features (see HDF5.jl issue #1079 for details). If you build PETSc from source, you may need to disable these features:\n\n./configure --with-hdf5=0 --with-ssl=0 [other options...]\n\nSee the PETSc installation documentation for complete configuration options.","category":"section"},{"location":"installation/#Package-Installation","page":"Installation","title":"Package Installation","text":"","category":"section"},{"location":"installation/#Basic-Installation","page":"Installation","title":"Basic Installation","text":"using Pkg\nPkg.add(\"MultiGridBarrierPETSc\")","category":"section"},{"location":"installation/#Development-Installation","page":"Installation","title":"Development Installation","text":"To install the development version:\n\nusing Pkg\nPkg.add(url=\"https://github.com/sloisel/MultiGridBarrierPETSc.jl\")\n\nOr clone and develop locally:\n\ngit clone https://github.com/sloisel/MultiGridBarrierPETSc.jl\ncd MultiGridBarrierPETSc.jl\njulia --project -e 'using Pkg; Pkg.instantiate()'","category":"section"},{"location":"installation/#Verification","page":"Installation","title":"Verification","text":"Test your installation with 4 MPI ranks:\n\ncd MultiGridBarrierPETSc.jl\njulia --project=. -e 'using Pkg; Pkg.test()'\n\nAll tests should pass. Expected output:\n\nTest Summary:                    | Pass  Total\nMultiGridBarrierPETSc Test Suite |   ##     ##","category":"section"},{"location":"installation/#Initialization-Pattern","page":"Installation","title":"Initialization Pattern","text":"tip: Initialization Pattern\nLoad the package first, then initialize:\n\n# ✓ CORRECT\nusing MultiGridBarrierPETSc\nMultiGridBarrierPETSc.Init()  # Initialize MPI and PETSc after loading package\n\n# ✗ WRONG - Init() must be called before using any functions\nusing MultiGridBarrierPETSc\n# Missing MultiGridBarrierPETSc.Init() - will fail when calling functions","category":"section"},{"location":"installation/#Running-MPI-Programs","page":"Installation","title":"Running MPI Programs","text":"","category":"section"},{"location":"installation/#Interactive-REPL-(Single-Rank)","page":"Installation","title":"Interactive REPL (Single Rank)","text":"For development and testing on a single rank:\n\nusing MultiGridBarrierPETSc\nMultiGridBarrierPETSc.Init()\n\n# Your code here...","category":"section"},{"location":"installation/#Multi-Rank-Execution","page":"Installation","title":"Multi-Rank Execution","text":"For distributed execution, create a script file (e.g., my_program.jl):\n\nusing SafePETSc  # For io0()\nusing MultiGridBarrierPETSc\nMultiGridBarrierPETSc.Init()\n\n# Your parallel code here\nsol = fem2d_petsc_solve(Float64; L=3, p=1.0)\nprintln(io0(), \"Solution computed!\")\n\nRun with Julia's MPI launcher:\n\njulia -e 'using MPI; run(`$(MPI.mpiexec()) -n 4 $(Base.julia_cmd()) my_program.jl`)'\n\nThis uses MPI.mpiexec() to get the correct MPI launcher configured for your Julia installation, avoiding compatibility issues with system mpiexec.\n\ntip: Julia Options\nAdd --project, --threads, or other Julia options as needed for your environment:julia --project=/path/to/project -e 'using MPI; run(`$(MPI.mpiexec()) -n 4 $(Base.julia_cmd()) --project=/path/to/project my_program.jl`)'\n\ntip: Output from Rank 0 Only\nUse io0() from SafePETSc for output to avoid duplicate messages:println(io0(), \"This prints once from rank 0\")","category":"section"},{"location":"installation/#Troubleshooting","page":"Installation","title":"Troubleshooting","text":"","category":"section"},{"location":"installation/#MPI-Issues","page":"Installation","title":"MPI Issues","text":"If you see MPI-related errors, try rebuilding MPI.jl:\n\nusing Pkg; Pkg.build(\"MPI\")","category":"section"},{"location":"installation/#PETSc/MUMPS-Issues","page":"Installation","title":"PETSc/MUMPS Issues","text":"If PETSc fails to load:\n\nRebuild SafePETSc: julia -e 'using Pkg; Pkg.build(\"SafePETSc\")'\nCheck PETSc installation: julia -e 'using MultiGridBarrierPETSc; MultiGridBarrierPETSc.Init(); println(\"OK\")'","category":"section"},{"location":"installation/#Test-Failures","page":"Installation","title":"Test Failures","text":"If tests fail:\n\nEnsure you're using at least Julia 1.10 (LTS version)\nCheck all dependencies are installed: Pkg.status()\nRun with verbose output: Pkg.test(\"MultiGridBarrierPETSc\"; test_args=[\"--verbose\"])","category":"section"},{"location":"installation/#Next-Steps","page":"Installation","title":"Next Steps","text":"Once installed, proceed to the User Guide to learn how to use the package.","category":"section"},{"location":"","page":"Home","title":"Home","text":"using Markdown\nusing Pkg\nusing MultiGridBarrierPETSc\nv = string(pkgversion(MultiGridBarrierPETSc))\nmd\"# MultiGridBarrierPETSc.jl $v\"\n\nA Julia package that bridges MultiGridBarrier.jl and SafePETSc.jl for distributed multigrid barrier computations.","category":"section"},{"location":"#Overview","page":"Home","title":"Overview","text":"MultiGridBarrierPETSc.jl extends the MultiGridBarrier.jl package to work with PETSc's distributed Mat and Vec types through SafePETSc.jl. This enables efficient parallel computation of multigrid barrier methods across multiple MPI ranks.","category":"section"},{"location":"#Key-Features","page":"Home","title":"Key Features","text":"1D, 2D, and 3D Support: Full support for 1D elements, 2D triangular, and 3D hexahedral finite elements\nSeamless Integration: Drop-in replacement for MultiGridBarrier's native types\nDistributed Computing: Leverage PETSc's distributed linear algebra for large-scale problems\nType Conversion: Easy conversion between native Julia arrays and PETSc distributed types\nMPI-Aware: All operations correctly handle MPI collective requirements\nMUMPS Solver: Automatically configures MUMPS direct solver for accurate Newton iterations","category":"section"},{"location":"#Quick-Example","page":"Home","title":"Quick Example","text":"Solve a 2D p-Laplace problem with distributed PETSc types. Save this code to example.jl:\n\nusing MPI\nusing MultiGridBarrierPETSc\nusing MultiGridBarrier\nusing PyPlot\nMultiGridBarrierPETSc.Init()\n\n# Solve with PETSc distributed types (L=3 refinement levels)\nsol_petsc = fem2d_petsc_solve(Float64; L=3, p=1.0, verbose=false)\n\n# Convert to native types for visualization\nsol_native = petsc_to_native(sol_petsc)\n\n# Only rank 0 creates the plot\nrank = MPI.Comm_rank(MPI.COMM_WORLD)\nif rank == 0\n    plot(sol_native)\n    savefig(\"solution.png\")\n    println(\"Plot saved to solution.png\")\nend\n\nRun with 4 MPI ranks using Julia's MPI launcher:\n\njulia -e 'using MPI; run(`$(MPI.mpiexec()) -n 4 $(Base.julia_cmd()) example.jl`)'\n\nThis command uses MPI.mpiexec() to get the correct MPI launcher configured for your Julia installation, avoiding compatibility issues with system mpiexec. Add --project or other Julia options as needed for your environment.\n\n(Image: 2D p-Laplace Solution)\n\nThe plot above shows the solution to a 2D p-Laplace problem computed using distributed PETSc matrices across 4 MPI ranks, then converted back to native Julia types for visualization.","category":"section"},{"location":"#Documentation-Contents","page":"Home","title":"Documentation Contents","text":"Pages = [\"installation.md\", \"guide.md\", \"api.md\"]\nDepth = 2","category":"section"},{"location":"#Package-Ecosystem","page":"Home","title":"Package Ecosystem","text":"This package is part of a larger ecosystem:\n\nMultiGridBarrier.jl: Core multigrid barrier method implementation (2D)\nMultiGridBarrier3d.jl: 3D hexahedral finite element extension\nSafePETSc.jl: Safe PETSc bindings with automatic memory management\nMPI.jl: Julia MPI bindings for distributed computing","category":"section"},{"location":"#Requirements","page":"Home","title":"Requirements","text":"Julia 1.10 or later (LTS version)\nMPI installation (OpenMPI, MPICH, or Intel MPI)\nPETSc with MUMPS support (automatically configured)\nAt least 4 MPI ranks recommended for testing","category":"section"},{"location":"#Citation","page":"Home","title":"Citation","text":"If you use this package in your research, please cite:\n\n@software{multigridbarrierpetsc,\n  author = {Loisel, Sebastien},\n  title = {MultiGridBarrierPETSc.jl: Distributed Multigrid Barrier Methods},\n  year = {2024},\n  url = {https://github.com/sloisel/MultiGridBarrierPETSc.jl}\n}","category":"section"},{"location":"#License","page":"Home","title":"License","text":"This package is licensed under the MIT License.","category":"section"}]
}
